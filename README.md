# Automated Replications: Scalable, Rapid, and Updating Crowdsourcing of Literature

This repository is an open collaboration space for curating papers on new approaches that leverage computational tools, big data sources, and machine learning to evaluate the replicability and reproducibility of research results across sciences. 

Although diverse on the surface, what makes automated replications somewhat coherent, and distinct from the gold standard techniques such as direct replications, is the potential to be scalable, rapid, and constantly updating.  

**Scalable:** can simultaneously evaluate numerous published studies, hypotheses, results, and claims against purpose-build (e.g., prediction markets) or repurposed (e.g., high-throughput experiments) verification data. 

**Rapid:** can efficiently screen research publications to promptly uncover false positive results, possibly before they propagate in the literature. 

**Updating:** can incorporate next results in a continuously updating manner. 

Salganik's distinction in [Bit By Bit: Social Research in the Digital Age](https://www.bitbybitbook.com) between readymade and custommade data is useful here. We extend it to tentatively divide automated approaches to research evaluation into four categories:
- **Reuse** (e.g., aggregation of published results aka meta-analysis or scalable computational reproducibility)
- **Repurpose** (e.g., use of Genome-wide association studies and high-throughout experiments to evaluate published results)
- **Crowdsource** (e.g., use of prediction markets to aggregate individual beliefs)
- **Simulate** (e.g., computer simulations)

## Review

- [Reproducibility in Cancer Biology: Making sense of replications](https://elifesciences.org/articles/23383)
- [What does research reproducibility mean?](http://stm.sciencemag.org/content/8/341/341ps12)
- [DARPA Wants to Build a BS Detector for Science](https://www.wired.com/story/darpa-bs-detector-science/)
- [“PICkLE” – Path to Iterative Confidence Level](https://osf.io/8wnzs/)

## Reuse (Reproducibility)

- [Reproducibility of computational workflows is automated using continuous analysis](https://www.nature.com/articles/nbt.3780)
- [An empirical analysis of journal policy effectiveness for computational reproducibility](http://www.pnas.org/content/115/11/2584) 

## Repurpose

- [Centralized “big science” communities more likely generate non-replicable results](https://arxiv.org/pdf/1801.05042.pdf)
- [Large-scale Efforts to Massively Replicate Reported Candidate-gene Associations (Table 1)](https://www.gwern.net/docs/statistics/decision/2011-ioannidis.pdf)
- [Repeatability of published microarray gene expression analyses](https://www.nature.com/articles/ng.295)

## Crowdsource

- [Using prediction markets to estimate the reproducibility of scientific research](http://www.pnas.org/content/early/2015/11/04/1516179112)
- [Evaluating replicability of laboratory experiments in economics](http://science.sciencemag.org/content/early/2016/03/02/science.aaf0918)

## Simulate

- [Reproducibility of preclinical animal research improves with heterogeneity of study samples](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2003693)
- [Randomly auditing research labs could be an affordable way to improve research quality: A simulation study](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0195613)
- [The natural selection of bad science](http://rsos.royalsocietypublishing.org/content/3/9/160384)

## Initiatives & Funding 
- [Systematizing Confidence in Open Research and Evidence (SCORE)](https://events.sa-meetings.com/ehome/index.php?eventid=340598&)
- [NIH STRATEGIC PLAN FOR DATA SCIENCE](https://datascience.nih.gov/sites/default/files/NIH_Strategic_Plan_for_Data_Science_Final_508.pdf)

The idea of crowdsourcing literature was inspired by [pimentel](https://github.com/pimentel/deep_learning_papers/blob/master/README.md) & [greenelab](https://github.com/greenelab/deep-review). 

## License
This project is licensed under the GNU General Public License v3.0—see the [LICENSE.md](LICENSE.md) file for details
